{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from lib.utilities import *\n",
    "from lib.experiments.an4_speech_encoder_decoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = HYPERPARAMETERS({\n",
    "    'EXPERIMENT': 'AN4',\n",
    "    'DESCRIPTION': 'Transformer model',\n",
    "    'TIMESTAMP': HYPERPARAMETERS.create_timestamp(),\n",
    "\n",
    "    'MODEL_NAME': 'AN4_CNN_TRANSFORMER',\n",
    "\n",
    "    'PRELOAD_MODEL_PATH': None, #'AN4_CNN_TRANSFORMER.tar',\n",
    "\n",
    "    'ROOT_DIR': '/Volumes/SSD1',\n",
    "    'MANIFESTS': ['manifest.json'],  # , 'sts_manifest_pseudo.json'],\n",
    "\n",
    "    'TARGET_ENCODING': 'sts',  # ' ctc\n",
    "\n",
    "    'BATCH_SIZE': 20,\n",
    "    'NUM_WORKERS': 8,\n",
    "\n",
    "    'RNN_HIDDEN_SIZE': 256,\n",
    "    'RNN_NUM_LAYERS': 2,\n",
    "    'RNN_DROPOUT': 0.5,\n",
    "    'CNN_DROPOUT': 0.5,\n",
    "    'BIDIRECTIONAL': True,\n",
    "\n",
    "    'LR': 0.0003,\n",
    "    'LR_LAMBDA': lambda epoch: max(math.pow(0.78, math.floor((1 + epoch) / 200.0)), 0.01),\n",
    "    'WEIGHT_DECAY': 0,\n",
    "    'MOMENTUM': 0.9,\n",
    "    'NESTEROV': True,\n",
    "\n",
    "    'TEACHER_FORCING_RATIO': 0.5,\n",
    "\n",
    "    'LABEL_SMOOTHING' : 0.2,\n",
    "\n",
    "    'MAX_GRAD_NORM': 400,\n",
    "\n",
    "    'MAX_EPOCHS': 200,\n",
    "\n",
    "    'STOPPING_PATIENCE': 80,\n",
    "\n",
    "    'CHECKPOINT_INTERVAL': 10,\n",
    "    'CHECKPOINT_RESTORE': False,\n",
    "\n",
    "    'USE_CUDA': torch.cuda.is_available(),\n",
    "\n",
    "    'SEED': 123456,\n",
    "\n",
    "    'DATASET_MEAN_STD': (0.060487103, 0.16884679),\n",
    "\n",
    "    'NORMALIZE_DB': -40,\n",
    "    'NORMALIZE_MAX_GAIN': 300,\n",
    "\n",
    "    'MIN_MAX_AUDIO_DURATION': None,  # (1, 15),\n",
    "    'MIN_MAX_TRANSCRIPT_LEN': None,  # (0, 15),\n",
    "    'MIN_TRANSCRIPT_CONFIDENCE': None,  # 0.95,\n",
    "\n",
    "    'AUDIO_SAMPLE_RATE': 16000,\n",
    "\n",
    "    'SPECT_WINDOW_SIZE': 0.02,\n",
    "    'SPECT_WINDOW_STRIDE': 0.01,\n",
    "    'SPECT_WINDOW': 'hamming',\n",
    "\n",
    "    'AUGMENTATION_PROBABILITY': 0.0,\n",
    "\n",
    "    'NOISE_BG_PROBABILITY': 0.4,\n",
    "    'NOISE_BG_LEVELS': (0.0, 0.5),\n",
    "    'NOISE_BG_DIR': '/Volumes/SSD1/BACKGROUND_NOISE',\n",
    "\n",
    "    'AUDIO_PITCH_PROBABILITY': 0.4,\n",
    "    'AUDIO_PITCH_PM': 4,\n",
    "\n",
    "    'AUDIO_SPEED_PROBABILITY': 0.4,\n",
    "    'AUDIO_SPEED_LOW_HIGH': (0.9, 1.1),\n",
    "\n",
    "    'AUDIO_DYNAMIC_PROBABILITY': 0.4,\n",
    "    'AUDIO_DYNAMIC_LOW_HIGH': (0.5, 1.1),\n",
    "\n",
    "    'AUDIO_SHIFT_PROBABILITY': 0.4,\n",
    "    'AUDIO_SHIFT_MIN_MAX': (-5, 5),\n",
    "\n",
    "    'AUDIO_NOISE_PROBABILITY': 0.4,\n",
    "    'AUDIO_NOISE_LEVELS': (0.0, 0.5),\n",
    "    'AUDIO_NOISE_COLORS': ['white', 'pink', 'blue', 'brown', 'violet'],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, vocab = create_data_pipelines(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 1, 161, 81]),\n",
       " torch.Size([20, 7]),\n",
       " torch.Size([20]),\n",
       " torch.Size([20]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for inputs_cpu, labels_cpu, input_sizes_cpu, label_sizes_cpu, _ in train_loader:\n",
    "    break\n",
    "\n",
    "inputs_cpu.shape, labels_cpu.shape, input_sizes_cpu.shape, label_sizes_cpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskModule(nn.Module):\n",
    "    def __init__(self, seq_module):\n",
    "        \"\"\"\n",
    "        Adds padding to the output of the module based on the given lengths. This is to ensure that the\n",
    "        results of the model do not change when batch sizes change during inference.\n",
    "        Input needs to be in the shape of (BxCxDxT)\n",
    "        \"\"\"\n",
    "        super(MaskModule, self).__init__()\n",
    "        self.seq_module = seq_module\n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        \"\"\"\n",
    "        Input of size BxCxDxT\n",
    "        \"\"\"\n",
    "        lengths = self.get_seq_lens(x_lengths)\n",
    "        for module in self.seq_module:\n",
    "            x = module(x)\n",
    "            mask = torch.ByteTensor(x.size()).fill_(0)\n",
    "            if x.is_cuda:\n",
    "                mask = mask.cuda()\n",
    "            for i, length in enumerate(lengths):\n",
    "                length = length.item()\n",
    "                if (mask[i].size(2) - length) > 0:\n",
    "                    mask[i].narrow(2, length, mask[i].size(2) - length).fill_(1)\n",
    "            x = x.masked_fill(mask, 0)\n",
    "        return x, lengths\n",
    "\n",
    "    def get_seq_lens(self, input_lengths):\n",
    "        \"\"\"\n",
    "        Given a 1D Tensor or Variable containing integer sequence lengths, return a 1D tensor or variable\n",
    "        containing the size sequences that will be output by the network.\n",
    "        \"\"\"\n",
    "        seq_len = input_lengths.cpu().int()\n",
    "        for m in self.seq_module.modules():\n",
    "            if type(m) == nn.modules.conv.Conv2d:\n",
    "                seq_len = ((seq_len + 2 * m.padding[1] - m.dilation[1] * (m.kernel_size[1] - 1) - 1) / m.stride[1] + 1)\n",
    "            elif type(m) == nn.modules.pooling.MaxPool2d: \n",
    "                seq_len = ((seq_len + 2 * m.padding[1] - m.dilation * (m.kernel_size[1] - 1) - 1) / m.stride[1] + 1)                \n",
    "            elif type(m) == torch.nn.modules.pooling.AvgPool2d:\n",
    "                seq_len = ((seq_len + 2 * m.padding[1] - m.kernel_size[1]) / m.stride[1] + 1)   \n",
    "                \n",
    "        return seq_len.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, dropout=0.5, initialize=None):\n",
    "        super(CNN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.initialize = initialize\n",
    "        self.inplanes = 32\n",
    "\n",
    "        self.conv1 = MaskModule(nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5), bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True),\n",
    "            nn.Dropout(dropout)\n",
    "        ))\n",
    "\n",
    "        self.conv2 = MaskModule(nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5), bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True),\n",
    "            nn.Dropout(dropout)\n",
    "        ))\n",
    "\n",
    "        self.bb1 = MaskModule(self._make_layer(BasicBlock, 64, 2, stride=(2, 1)))\n",
    "        self.bb2 = MaskModule(self._make_layer(BasicBlock, 128, 2, stride=(2, 1)))\n",
    "        self.bb3 = MaskModule(self._make_layer(BasicBlock, 256, 2, stride=(2, 1)))\n",
    "        self.bb4 = MaskModule(self._make_layer(BasicBlock, 256, 2, stride=(2, 1)))\n",
    "\n",
    "        self.conv3 = MaskModule(nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 1), padding=(0, 0), bias=False),\n",
    "            nn.BatchNorm2d(256, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ))\n",
    "\n",
    "        if not self.initialize is None:\n",
    "            self.initialize(self)\n",
    "\n",
    "    def forward(self, inputs, input_sizes):\n",
    "        outputs, output_sizes = self.conv1(inputs, input_sizes)\n",
    "\n",
    "        outputs, output_sizes = self.conv2(outputs, output_sizes)\n",
    "\n",
    "        outputs, output_sizes  = self.bb1(outputs, output_sizes)\n",
    "        outputs, output_sizes  = self.bb2(outputs, output_sizes)\n",
    "        outputs, output_sizes  = self.bb3(outputs, output_sizes)\n",
    "        outputs, output_sizes  = self.bb4(outputs, output_sizes)\n",
    "\n",
    "        outputs, output_sizes  = self.conv3(outputs, output_sizes)\n",
    "        \n",
    "        outputs = outputs.squeeze(2).transpose(1,2)\n",
    "        \n",
    "        return outputs, output_sizes\n",
    "    \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, dropout=0.5, initialize=None):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.initialize = initialize\n",
    "\n",
    "#         self.conv = MaskModule(nn.Sequential(\n",
    "#             nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5), bias=False),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.Hardtanh(0, 20, inplace=True),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5), bias=False),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.Hardtanh(0, 20, inplace=True),\n",
    "#             nn.Dropout(dropout)\n",
    "#         ))\n",
    "\n",
    "#         if self.initialize is not None:\n",
    "#             self.initialize(self)\n",
    "\n",
    "#     def forward(self, x, lengths):\n",
    "\n",
    "#         output_lengths = self.get_seq_lens(lengths)\n",
    "\n",
    "#         x, _ = self.conv(x, output_lengths)\n",
    "\n",
    "#         sizes = x.size()\n",
    "#         x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # Collapse feature dimension\n",
    "#         x = x.transpose(1, 2).transpose(0, 1).contiguous()  # TxNxH\n",
    "\n",
    "#         return x, output_lengths\n",
    "\n",
    "#     def get_seq_lens(self, input_length):\n",
    "#         \"\"\"\n",
    "#         Given a 1D Tensor or Variable containing integer sequence lengths, return a 1D tensor or variable\n",
    "#         containing the size sequences that will be output by the network.\n",
    "#         :param input_length: 1D Tensor\n",
    "#         :return: 1D Tensor scaled by model\n",
    "#         \"\"\"\n",
    "#         seq_len = input_length.cpu().int()\n",
    "#         for m in self.conv.modules():\n",
    "#             if type(m) == nn.modules.conv.Conv2d:\n",
    "#                 seq_len = ((seq_len + 2 * m.padding[1] - m.dilation[1] * (m.kernel_size[1] - 1) - 1) / m.stride[1] + 1)\n",
    "#         return seq_len.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 1, 161, 81]),\n",
       " torch.Size([20]),\n",
       " torch.Size([20, 39, 256]),\n",
       " torch.Size([20]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_cpu = CNN(dropout=H.CNN_DROPOUT, initialize=torch_weight_init)\n",
    "\n",
    "cnn_outputs_cpu, cnn_output_sizes_cpu = cnn_cpu(inputs_cpu, input_sizes_cpu)\n",
    "\n",
    "inputs_cpu.shape, input_sizes_cpu.shape, cnn_outputs_cpu.shape, cnn_output_sizes_cpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#torch.Size([20, 269, 256]) torch.Size([256, 256]) 269\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, droput, len_max=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.droput = droput\n",
    "        self.len_max = len_max\n",
    "        \n",
    "        position = torch.arange(0.0, self.len_max)\n",
    "        num_timescales = self.d_model // 2\n",
    "        log_timescale_increment = math.log(10000) / (num_timescales - 1)\n",
    "        inv_timescales = torch.exp(torch.arange(0.0, num_timescales) * -log_timescale_increment)\n",
    "        scaled_time = position.unsqueeze(1) * inv_timescales.unsqueeze(0)\n",
    "        pos_emb = torch.cat((torch.sin(scaled_time), torch.cos(scaled_time)), 1)\n",
    "\n",
    "        # wrap in a buffer so that model can be moved to GPU\n",
    "        self.register_buffer('pos_emb', pos_emb)\n",
    "\n",
    "        self.drop = nn.Dropout(self.droput)\n",
    "\n",
    "    def forward(self, word_emb):\n",
    "        len_seq = word_emb.size(1)\n",
    "        out = word_emb + self.pos_emb[:len_seq, :]\n",
    "        out = self.drop(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, droput):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.droput = droput\n",
    "\n",
    "        self.d_head = d_model // self.num_heads\n",
    "\n",
    "        self.fc_query = nn.Linear(self.d_model, self.num_heads * self.d_head, bias=False)\n",
    "        self.fc_key = nn.Linear(self.d_model, self.num_heads * self.d_head, bias=False)\n",
    "        self.fc_value = nn.Linear(self.d_model, self.num_heads * self.d_head, bias=False)\n",
    "\n",
    "        self.fc_concat = nn.Linear(self.num_heads * self.d_head, self.d_model, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(self.droput)\n",
    "        self.dropout = nn.Dropout(self.droput)\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    def _prepare_proj(self, x):\n",
    "        \"\"\"Reshape the projectons to apply softmax on each head\n",
    "        \"\"\"\n",
    "        b, l, d = x.size()\n",
    "        return x.view(b, l, self.num_heads, self.d_head).transpose(1, 2).contiguous().view(b * self.num_heads, l,\n",
    "                                                                                           self.d_head)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        b, len_query = query.size(0), query.size(1)\n",
    "        len_key = key.size(1)\n",
    "\n",
    "        # project inputs to multi-heads\n",
    "        proj_query = self.fc_query(query)  # batch_size x len_query x h*d_head\n",
    "        proj_key = self.fc_key(key)  # batch_size x len_key x h*d_head\n",
    "        proj_value = self.fc_value(value)  # batch_size x len_key x h*d_head\n",
    "\n",
    "        # prepare the shape for applying softmax\n",
    "        proj_query = self._prepare_proj(proj_query)  # batch_size*h x len_query x d_head\n",
    "        proj_key = self._prepare_proj(proj_key)  # batch_size*h x len_key x d_head\n",
    "        proj_value = self._prepare_proj(proj_value)  # batch_size*h x len_key x d_head\n",
    "\n",
    "        # get dotproduct softmax attns for each head\n",
    "        attns = torch.bmm(proj_query, proj_key.transpose(1, 2))  # batch_size*h x len_query x len_key\n",
    "        attns = attns / math.sqrt(self.d_head)\n",
    "        attns = attns.view(b, self.num_heads, len_query, len_key)\n",
    "        attns = attns.masked_fill_(mask.unsqueeze(1), -float('inf'))\n",
    "        attns = self.softmax(attns.view(-1, len_key))\n",
    "\n",
    "        # return mean attention from all heads as coverage\n",
    "        coverage = torch.mean(attns.view(b, self.num_heads, len_query, len_key), dim=1)\n",
    "\n",
    "        attns = self.attn_dropout(attns)\n",
    "        attns = attns.view(b * self.num_heads, len_query, len_key)\n",
    "\n",
    "        # apply attns on value\n",
    "        out = torch.bmm(attns, proj_value)  # batch_size*h x len_query x d_head\n",
    "        out = out.view(b, self.num_heads, len_query, self.d_head).transpose(1, 2).contiguous()\n",
    "\n",
    "        out = self.fc_concat(out.view(b, len_query, self.num_heads * self.d_head))\n",
    "\n",
    "        out = self.dropout(out).add_(query)\n",
    "        out = self.norm(out)\n",
    "        return out, coverage\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.drop = nn.Dropout(self.dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.fc(inputs)\n",
    "        out = self.drop(out).add_(inputs)\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, dropout, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = MultiHeadAttention(self.num_heads, self.d_model, self.dropout)\n",
    "\n",
    "        self.ff = PositionwiseFeedForward(self.d_model, self.d_ff, self.dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        out, _ = self.attention(query, key, value, mask)\n",
    "        out = self.ff(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, dropout, d_ff):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention_tgt = MultiHeadAttention(self.num_heads, self.d_model, self.dropout)\n",
    "\n",
    "        self.attention_src = MultiHeadAttention(self.num_heads, self.d_model, self.dropout)\n",
    "\n",
    "        self.ff = PositionwiseFeedForward(d_model, self.d_ff, self.dropout)\n",
    "\n",
    "    def forward(self, query, key, value, context, mask_tgt, mask_src):\n",
    "        out, _ = self.attention_tgt(query, key, value, mask_tgt)\n",
    "        out, coverage = self.attention_src(out, context, context, mask_src)\n",
    "        out = self.ff(out)\n",
    "        return out, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, dropout, d_ff, num_layers=6, padding_idx=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.padding_idx = padding_idx\n",
    "        self.num_layers = num_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.cnn = CNN(dropout=0.5)\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(self.d_model, self.dropout, len_max=1024)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(self.num_heads, self.d_model, self.dropout, self.d_ff) for _ in range(self.num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, input_sizes):\n",
    "        context, context_size = self.cnn(inputs, input_sizes)  # batch_size x len_src x d_model\n",
    "        \n",
    "        mask_src = self.get_mask(context_size).unsqueeze(1)\n",
    "        mask_src = mask_src.cuda() if next(self.parameters()).is_cuda else mask_src\n",
    "        self.mask = mask_src\n",
    "        \n",
    "        context = self.pos_emb(context)\n",
    "\n",
    "        for _, layer in enumerate(self.layers):\n",
    "            context = layer(context, context, context, mask_src)  # batch_size x len_src x d_model\n",
    "        return context, mask_src\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mask(lengths):\n",
    "        batch_size = lengths.numel()\n",
    "        mask = (torch.arange(0, lengths.max()).type_as(lengths).repeat(batch_size, 1).gt(lengths.unsqueeze(1)))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 39, 256]), torch.Size([20, 1, 39]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_cpu = Encoder(num_heads=8, d_model=256, dropout=0.1, d_ff=1024, num_layers=6, \n",
    "                     padding_idx=0)\n",
    "                     \n",
    "context_cpu, mask_src_cpu = encode_cpu(inputs_cpu, input_sizes_cpu)\n",
    "\n",
    "context_cpu.shape, mask_src_cpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_heads, d_model, dropout, d_ff, num_layers=6, padding_idx=0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.padding_idx = padding_idx\n",
    "        self.num_layers = num_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model, padding_idx=self.padding_idx)\n",
    "\n",
    "        self.pos_emb = PositionalEncoding(self.d_model, self.dropout, len_max=d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(self.num_heads, self.d_model, self.dropout, self.d_ff) for _ in range(self.num_layers)]\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.d_model, self.vocab_size, bias=True)\n",
    "\n",
    "        # tie weight between word embedding and generator\n",
    "        self.fc.weight = self.embedding.weight\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # pre-save a mask to avoid future information in self-attentions in decoder\n",
    "        # save as a buffer, otherwise will need to recreate it and move to GPU during every call\n",
    "        mask = torch.ByteTensor(np.triu(np.ones((self.d_model, self.d_model)), k=1).astype('uint8'))\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, tgt, context, mask_src):\n",
    "        out = self.embedding(tgt)  # batch_size x len_tgt x d_model\n",
    "\n",
    "        out = self.pos_emb(out)\n",
    "\n",
    "        len_tgt = tgt.size(1)\n",
    "        mask_tgt = tgt.data.eq(self.padding_idx).unsqueeze(1) + self.mask[:len_tgt, :len_tgt]\n",
    "        mask_tgt = torch.gt(mask_tgt, 0)\n",
    "        for _, layer in enumerate(self.layers):\n",
    "            out, coverage = layer(out, out, out, context, mask_tgt, mask_src)  # batch_size x len_tgt x d_model\n",
    "\n",
    "        out = self.fc(out)  # batch_size x len_tgt x bpe_size\n",
    "\n",
    "        out = self.logsoftmax(out.view(-1, self.vocab_size))\n",
    "        return out, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, tgt_vocab, num_heads, d_model, dropout, d_ff, num_layers=6, padding_idx=1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.tgt_vocab_size = len(tgt_vocab)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.encode = Encoder(self.num_heads, self.d_model, self.dropout, self.d_ff,\n",
    "                              self.num_layers, self.padding_idx)\n",
    "        self.decode = Decoder(self.tgt_vocab_size, self.num_heads, self.d_model, self.dropout, self.d_ff,\n",
    "                              self.num_layers, self.padding_idx)\n",
    "\n",
    "    def forward(self, inputs, input_sizes, tgt):\n",
    "        \n",
    "        context, mask_src = self.encode(inputs, input_sizes,)\n",
    "        \n",
    "        outputs, _ = self.decode(tgt, context, mask_src)\n",
    "\n",
    "        probas = outputs.view(inputs.size(0), -1, self.tgt_vocab_size)\n",
    "        _, max_indices = torch.max(probas, 2)\n",
    "        tmp = max_indices.eq(self.tgt_vocab('<EOS>'))\n",
    "        tmp[:,-1] = 1\n",
    "        proba_sizes = torch.max(tmp, dim=1)[1] + 1\n",
    "\n",
    "        return probas, proba_sizes\n",
    "\n",
    "    def decode_greedy(self, inputs, input_sizes, labels=None, max_seq_length=50):\n",
    "        use_cuda = next(self.parameters()).is_cuda\n",
    "\n",
    "        idx_sos, idx_eos = self.tgt_vocab('<SOS>'), self.tgt_vocab('<EOS>')\n",
    "\n",
    "        context, mask_src = self.encode(inputs, input_sizes,)\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        decode_input = torch.ones(batch_size, 1).fill_(idx_sos).long()\n",
    "        decode_input = decode_input.cuda() if use_cuda else decode_input\n",
    "\n",
    "        dec_output_sizes = torch.LongTensor(batch_size).fill_(max_seq_length).long()\n",
    "        dec_output_sizes = dec_output_sizes.cuda() if use_cuda else dec_output_sizes\n",
    "\n",
    "        max_steps = labels.size(1) if labels is not None else max_seq_length + 1\n",
    "\n",
    "        dec_outputs = []\n",
    "        for step in range(max_steps):\n",
    "            outputs, _ = self.decode(decode_input, context, mask_src)\n",
    "            outputs = outputs.view(batch_size, -1, self.tgt_vocab_size)\n",
    "\n",
    "            dec_outputs.append(outputs[:, step, :].unsqueeze(1))\n",
    "\n",
    "            preds = torch.max(outputs[:, -1, :], dim=1)[1]\n",
    "\n",
    "            dec_output_sizes[preds.eq(idx_eos) * dec_output_sizes.gt(step)] = step\n",
    "            if labels is None and dec_output_sizes.le(step + 1).all():\n",
    "                break\n",
    "\n",
    "            decode_input = torch.cat([decode_input, preds.unsqueeze(1)], dim=1)\n",
    "\n",
    "        dec_outputs = torch.cat(dec_outputs, dim=1)\n",
    "\n",
    "        return dec_outputs, dec_output_sizes\n",
    "\n",
    "    def decode_beam(self, inputs, labels=None, max_seq_length=50, beam_size=64, alpha=0.1, beta=0.3):\n",
    "\n",
    "        context, mask_src = self.encode(inputs)\n",
    "\n",
    "        max_seq_len = labels.size(1) if labels is not None else max_seq_length\n",
    "\n",
    "        dec_outputs = []\n",
    "        for idx in range(context.size(0)):\n",
    "            target, _ = beam_search(self, self.tgt_vocab, context[idx].unsqueeze(0), mask_src[idx].unsqueeze(0),\n",
    "                                    beam_size=beam_size, alpha=alpha, beta=beta, max_seq_len=max_seq_len)\n",
    "            dec_outputs.append(target)\n",
    "\n",
    "        return dec_outputs\n",
    "\n",
    "\n",
    "def beam_search(model, vocab, context, mask_src, beam_size=64, alpha=0.1, beta=0.3, max_seq_len=64):\n",
    "    probas = []\n",
    "    preds = []\n",
    "    probs = []\n",
    "    coverage_penalties = []\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    idx_sos, idx_eos, idx_pad = vocab('<SOS>'), vocab('<EOS>'), 0\n",
    "\n",
    "    decode_inputs = torch.LongTensor([idx_sos]).unsqueeze(1)\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        decode_inputs = decode_inputs.cuda()\n",
    "\n",
    "    decode_outputs, coverage = model.decode(decode_inputs, context, mask_src)\n",
    "\n",
    "    scores, scores_idx = decode_outputs.view(-1).topk(beam_size)\n",
    "    beam_idx = scores_idx / vocab_size\n",
    "    pred_idx = (scores_idx - beam_idx * vocab_size).view(beam_size, -1)\n",
    "\n",
    "    decode_inputs = torch.cat((decode_inputs.repeat(beam_size, 1), pred_idx), 1)\n",
    "    context = context.repeat(beam_size, 1, 1)\n",
    "\n",
    "    remaining_beams = beam_size\n",
    "    for step in range(max_seq_len):\n",
    "        decode_outputs, coverage = model.decode(decode_inputs, context, mask_src)\n",
    "\n",
    "        decode_outputs = decode_outputs.view(remaining_beams, -1, vocab_size)\n",
    "        decode_outputs = scores.unsqueeze(1) + decode_outputs[:, -1, :]\n",
    "        scores, scores_idx = decode_outputs.view(-1).topk(remaining_beams)\n",
    "\n",
    "        beam_idx = scores_idx / vocab_size\n",
    "        pred_idx = (scores_idx - beam_idx * vocab_size).view(remaining_beams, -1)\n",
    "\n",
    "        decode_inputs = torch.cat((decode_inputs[beam_idx], pred_idx), 1)\n",
    "\n",
    "        index = decode_inputs[:, -1].eq(idx_eos) + decode_inputs[:, -1].eq(idx_pad)\n",
    "        finished = index.nonzero().flatten()\n",
    "        continue_idx = (index ^ 1).nonzero().flatten()\n",
    "\n",
    "        for idx in finished:\n",
    "            probas.append(scores[idx].item())\n",
    "            preds.append(decode_inputs[idx, :].tolist())\n",
    "            probs.append(coverage[idx, :, :])\n",
    "\n",
    "            atten_prob = torch.sum(coverage[idx, :, :], dim=0)\n",
    "            coverage_penalty = torch.log(atten_prob.masked_select(atten_prob.le(1)))\n",
    "            coverage_penalty = beta * torch.sum(coverage_penalty).item()\n",
    "            coverage_penalties.append(coverage_penalty)\n",
    "\n",
    "            remaining_beams -= 1\n",
    "\n",
    "        if len(continue_idx) > 0:\n",
    "            scores = scores.index_select(0, continue_idx)\n",
    "            decode_inputs = decode_inputs.index_select(0, continue_idx)\n",
    "            context = context.index_select(0, continue_idx)\n",
    "\n",
    "        if remaining_beams <= 0:\n",
    "            break\n",
    "\n",
    "    len_penalties = [math.pow(len(pred), alpha) for pred in preds]\n",
    "    #     final_scores = [probas[i] / len_penalties[i] + coverage_penalties[i] for i in range(len(preds))]\n",
    "    final_scores = [probas[i] / len_penalties[i] for i in range(len(preds))]\n",
    "\n",
    "    sorted_scores_arg = sorted(range(len(preds)), key=lambda i: -final_scores[i])\n",
    "\n",
    "    best_beam = sorted_scores_arg[0]\n",
    "\n",
    "    return preds[best_beam], probs[best_beam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 7, 32]) torch.Size([20])\n",
      "torch.Size([20, 7, 32]) torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "model_cpu = Transformer(vocab, num_heads=8, d_model=256, \n",
    "                        dropout=0.1, d_ff=1024, num_layers=6, padding_idx=0)\n",
    "\n",
    "outputs_cpu, output_sizes_cpu = model_cpu(inputs_cpu, input_sizes_cpu, labels_cpu)\n",
    "\n",
    "fix_embedding = torch.from_numpy(np.eye(len(vocab), 256).astype(np.float32))\n",
    "model_cpu.decode.embedding.weight = nn.Parameter(fix_embedding)\n",
    "model_cpu.decode.embedding.weight.requires_grad = False\n",
    "\n",
    "print(outputs_cpu.shape, output_sizes_cpu.shape)\n",
    "\n",
    "outputs_cpu, output_sizes_cpu = model_cpu.decode_greedy(inputs_cpu, input_sizes_cpu, labels_cpu)\n",
    "\n",
    "# print(outputs_cpu.shape, output_sizes_cpu.shape)\n",
    "\n",
    "# outputs_cpu, output_sizes_cpu = model_cpu.decode_beam(inputs_cpu, labels_cpu)\n",
    "\n",
    "print(outputs_cpu.shape, output_sizes_cpu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = Metric([('train_loss', np.inf), ('train_score', np.inf), ('valid_loss', np.inf), ('valid_score', 0),\n",
    "            ('train_lr', 0), ('valid_cer', np.inf)])\n",
    "\n",
    "model = Transformer(vocab, num_heads=8, d_model=256, \n",
    "                        dropout=0.1, d_ff=1024, num_layers=6, padding_idx=0)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        torch_weight_init(p)\n",
    "        \n",
    "fix_embedding = torch.from_numpy(np.eye(len(vocab), 256).astype(np.float32))\n",
    "model.decode.embedding.weight = nn.Parameter(fix_embedding)\n",
    "model.decode.embedding.weight.requires_grad = False\n",
    "\n",
    "if H.USE_CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "if H.PRELOAD_MODEL_PATH:\n",
    "    path = os.path.join(H.EXPERIMENT, H.PRELOAD_MODEL_PATH)\n",
    "    state = torch.load(path)\n",
    "    model.load_state_dict(state)\n",
    "    print(\"Preloaded model: {}\".format(path))\n",
    "\n",
    "# torch_weight_init(model.dec) \n",
    "    \n",
    "criterion = LabelSmoothingLoss(padding_idx=0, label_smoothing=H.LABEL_SMOOTHING)\n",
    "\n",
    "sts_decoder = STSDecoder(vocab)\n",
    "\n",
    "scorer = Scorer()\n",
    "\n",
    "optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, model.parameters())),\n",
    "                       amsgrad=False,\n",
    "                       betas=(0.9, 0.999),\n",
    "                       eps=1e-08,\n",
    "                       lr=H.LR,\n",
    "                       weight_decay=H.WEIGHT_DECAY)\n",
    "\n",
    "# optimizer = optim.SGD(list(filter(lambda p: p.requires_grad, model.parameters())),\n",
    "#                       lr=H.LR, weight_decay=H.WEIGHT_DECAY, momentum=H.MOMENTUM, nesterov=H.NESTEROV)\n",
    "\n",
    "stopping = Stopping(model, patience=H.STOPPING_PATIENCE)\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[H.LR_LAMBDA])\n",
    "\n",
    "# scheduler = NoamLRScheduler(optimizer, H.RNN_NUM_LAYERS, 10, 0.003)\n",
    "\n",
    "tlogger = TensorboardLogger(root_dir=H.EXPERIMENT, experiment_dir=H.TIMESTAMP)  # PytorchLogger()\n",
    "\n",
    "checkpoint = Checkpoint(model, optimizer, stopping, m,\n",
    "                        root_dir=H.EXPERIMENT, experiment_dir=H.TIMESTAMP, restore_from=-1,\n",
    "                        interval=H.CHECKPOINT_INTERVAL, verbose=0)\n",
    "\n",
    "# trainer = Trainer(model, train_loader, optimizer, scheduler, criterion, sts_decoder, scorer, H.MAX_GRAD_NORM)\n",
    "\n",
    "# evaluator = Evaluator(model, valid_loader, criterion, sts_decoder, scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(os.path.join(H.EXPERIMENT, H.MODEL_NAME + '.tar'))\n",
    "model.load_state_dict(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-95617a66c913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtotal_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtotal_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "epoch_start = 1\n",
    "if H.CHECKPOINT_RESTORE:\n",
    "    epoch_start = checkpoint.restore() + 1\n",
    "#     train_loader.batch_sampler.shuffle(epoch_start)\n",
    "\n",
    "epoch = epoch_start\n",
    "try:\n",
    "    epoch_itr = tlogger.set_itr(range(epoch_start, H.MAX_EPOCHS + 1))\n",
    "\n",
    "    for epoch in epoch_itr:\n",
    "        \n",
    "#         with DelayedKeyboardInterrupt():\n",
    "\n",
    "        model.train(True)\n",
    "\n",
    "        scheduler.step()\n",
    "    \n",
    "        train_lr = [float(param_group['lr']) for param_group in optimizer.param_groups][0]\n",
    "\n",
    "        total_size, total_loss, total_score = 0, 0.0, 0.0\n",
    "        for inputs, labels, input_sizes, label_sizes, _ in train_loader:\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            probas, proba_sizes = model(inputs, input_sizes, labels)\n",
    "\n",
    "            loss = criterion(probas, proba_sizes, labels.contiguous(), label_sizes)\n",
    "            total_loss += loss.item()      \n",
    "            \n",
    "            preds_seq, label_seq = sts_decoder(probas, proba_sizes, labels.contiguous(), label_sizes)\n",
    "            total_score += scorer(preds_seq, label_seq)\n",
    "            total_size += inputs.size(0)\n",
    "            1/0\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            if H.MAX_GRAD_NORM is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), H.MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "\n",
    "            del probas\n",
    "            del loss\n",
    "            \n",
    "        m.train_loss = total_loss / total_size\n",
    "        m.train_score = 1.0 - min(1.0, total_score / total_size)\n",
    "        m.train_lr = train_lr\n",
    "    \n",
    "        #-----------------------------------------------------------\n",
    "        \n",
    "#         model.eval()\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "\n",
    "#             hypotheses = []\n",
    "#             references = []\n",
    "#             total_size, total_loss, total_score = 0, 0.0, 0.0\n",
    "#             for inputs, labels, input_sizes, label_sizes, _ in valid_loader:\n",
    "#                 if next(model.parameters()).is_cuda:\n",
    "#                     inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "#                 probas, proba_sizes = model.decode_greedy(inputs, input_sizes, labels[:,1:], train_loader.dataset.max_seq_length)\n",
    "                \n",
    "#                 loss = criterion(probas, proba_sizes, labels[:,1:].contiguous(), label_sizes-1)\n",
    "#                 total_loss += loss.item()      \n",
    "\n",
    "#                 preds_seq, label_seq = sts_decoder(probas, proba_sizes, labels[:,1:].contiguous(), label_sizes-1)\n",
    "#                 total_score += scorer(preds_seq, label_seq)\n",
    "\n",
    "#                 total_size += inputs.size(0)\n",
    "                \n",
    "#             del probas\n",
    "#             del loss\n",
    "\n",
    "#         m.valid_loss = total_loss / total_size\n",
    "#         m.valid_score = 1.0 - min(1.0, total_score / total_size)\n",
    "\n",
    "        if checkpoint:\n",
    "            checkpoint.step(epoch)\n",
    "\n",
    "        stopping_flag = stopping.step(epoch, m.valid_loss, m.valid_score)\n",
    "\n",
    "        epoch_itr.log_values(m.train_loss, m.train_score, m.train_lr, m.valid_loss, m.valid_score,\n",
    "                             stopping.best_score_epoch, stopping.best_score)\n",
    "\n",
    "        if stopping_flag:\n",
    "            logger.info(\n",
    "                \"Early stopping at epoch: %d, score %f\" % (stopping.best_score_epoch, stopping.best_score))\n",
    "            break\n",
    "\n",
    "#             train_loader.batch_sampler.shuffle(epoch)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"Training interrupted at: {}\".format(epoch))\n",
    "    pass\n",
    "\n",
    "checkpoint.create(epoch)\n",
    "\n",
    "model.load_state_dict(stopping.best_score_state)\n",
    "torch.save(model.state_dict(), os.path.join(H.EXPERIMENT, H.MODEL_NAME + '.tar'))\n",
    "\n",
    "logger.info(repr(tlogger))\n",
    "logger.info(repr(stopping))\n",
    "logger.info(repr(checkpoint))\n",
    "\n",
    "logger.info(\"Training end.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1286, -0.0753, -0.6658, -0.1011, -0.4612, -0.1878, -0.0884],\n",
       "         [-0.9921, -1.0244, -0.3576, -0.0325, -0.9927, -1.2661, -0.2546],\n",
       "         [-0.3076, -0.8505, -0.7674, -0.9873, -0.1992, -1.0855, -0.1261],\n",
       "         [-0.8583, -0.2498, -0.0986, -0.0377, -0.5966, -0.0851, -0.4258],\n",
       "         [-0.3866, -0.7174, -0.3374, -0.9027, -0.9997, -1.2534, -1.2911],\n",
       "         [-0.1329, -0.0479, -0.4300, -0.5842, -0.7648, -0.0212, -0.5437],\n",
       "         [-1.0931, -0.1382, -1.6085, -0.4251, -0.8895, -0.1107, -0.0232],\n",
       "         [-0.0431, -0.0330, -0.6706, -0.1687, -0.2601, -0.3927, -0.9095],\n",
       "         [-0.0317, -0.6563, -0.8508, -0.2675, -0.6752, -0.8011, -0.4727],\n",
       "         [-0.1601, -0.0345, -0.1775, -0.3584, -0.0904, -0.7756, -1.3098],\n",
       "         [-0.0606, -0.0485, -0.1103, -0.6938, -0.5940, -0.5738, -1.3479],\n",
       "         [-0.6818, -1.0915, -0.2128, -1.4559, -0.3404, -0.4271, -0.0306],\n",
       "         [-0.5821, -0.0062, -0.1605, -0.2786, -0.3102, -0.3347, -0.3245],\n",
       "         [-0.3602, -0.0861, -0.2116, -0.8273, -0.7667, -1.3540, -0.9163],\n",
       "         [-0.7565, -0.1151, -0.3807, -0.2054, -0.3502, -1.0769, -0.9673],\n",
       "         [-0.8456, -1.2825, -0.6769, -1.2895, -0.2831, -0.3826, -0.7153],\n",
       "         [-0.5422, -0.9291, -0.4243, -0.2105, -0.4078, -0.0546, -0.4892],\n",
       "         [-0.6710, -0.1817, -0.3159, -0.0529, -0.7418, -0.2078, -0.0304],\n",
       "         [-0.3236, -0.6297, -0.8233, -0.6823, -1.3588, -0.4788, -0.3509],\n",
       "         [-0.1183, -0.4089, -0.1516, -0.1866, -1.7099, -0.3066, -0.0052]],\n",
       "        device='cuda:0', grad_fn=<MaxBackward0>),\n",
       " tensor([[28, 28, 28, 28,  6, 28, 28],\n",
       "         [13, 21, 28, 28, 28, 28,  6],\n",
       "         [28, 28, 28, 28,  9, 19,  6],\n",
       "         [28, 28, 28, 28, 28, 28, 28],\n",
       "         [15, 28, 28, 28,  9,  8, 28],\n",
       "         [28, 28, 28,  6, 28, 28,  8],\n",
       "         [ 8, 28, 14,  6,  3,  7, 28],\n",
       "         [28, 10, 27, 28, 28, 28, 28],\n",
       "         [28, 28, 28, 28, 19, 28, 19],\n",
       "         [28, 28, 28, 28, 28, 28,  3],\n",
       "         [28, 28,  9, 28, 10, 22,  5],\n",
       "         [28, 17,  6, 28, 28, 16,  6],\n",
       "         [28, 28, 10, 28, 28,  6, 23],\n",
       "         [28, 28, 28,  6, 28,  9, 26],\n",
       "         [28, 28, 28, 28,  6, 28,  6],\n",
       "         [ 7,  6, 28, 20, 28, 28,  6],\n",
       "         [ 6, 23, 28, 28, 28, 28, 28],\n",
       "         [26, 28, 28, 28, 28, 28, 28],\n",
       "         [28,  7, 22, 28, 13, 28, 19],\n",
       "         [ 2, 22,  3, 28,  6, 28, 28]], device='cuda:0'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(probas, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_transform = transforms.Compose([\n",
    "    AudioNormalizeDB(db=H.NORMALIZE_DB,\n",
    "                     max_gain_db=H.NORMALIZE_MAX_GAIN),\n",
    "    AudioSpectrogram(sample_rate=H.AUDIO_SAMPLE_RATE,\n",
    "                     window_size=H.SPECT_WINDOW_SIZE,\n",
    "                     window_stride=H.SPECT_WINDOW_STRIDE,\n",
    "                     window=H.SPECT_WINDOW),\n",
    "    AudioNormalize(),\n",
    "    FromNumpyToTensor(tensor_type=torch.FloatTensor)\n",
    "])\n",
    "\n",
    "label_transform = transforms.Compose([\n",
    "    TranscriptEncodeSTS(vocab),\n",
    "    FromNumpyToTensor(tensor_type=torch.LongTensor)\n",
    "])\n",
    "\n",
    "test_dataset = AudioDataset(os.path.join(H.ROOT_DIR, H.EXPERIMENT), manifests_files=H.MANIFESTS, datasets=\"test\",\n",
    "                            transform=audio_transform, label_transform=label_transform, max_data_size=None,\n",
    "                            sorted_by='recording_duration')\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=H.BATCH_SIZE, num_workers=H.NUM_WORKERS,\n",
    "                                          shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "logger.info(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pre = Transformer(vocab, num_heads=8, d_model=256, \n",
    "                        dropout=0.1, d_ff=1024, num_layers=6, padding_idx=0)\n",
    "if H.USE_CUDA:\n",
    "    model_pre.cuda()\n",
    "\n",
    "state = torch.load(os.path.join(H.EXPERIMENT, H.MODEL_NAME + '.tar'))\n",
    "model_pre.load_state_dict(state)\n",
    "\n",
    "sts_decoder = STSDecoder(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.28 s, sys: 154 ms, total: 4.44 s\n",
      "Wall time: 4.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "model_pre.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    for inputs, labels, input_sizes, label_sizes, _ in test_loader:\n",
    "        if next(model_pre.parameters()).is_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()        \n",
    " \n",
    "        probas, proba_sizes = model_pre.decode_greedy( inputs, input_sizes.cuda(), labels[:,1:], test_loader.dataset.max_seq_length)\n",
    "        \n",
    "        preds_seq, label_seq = sts_decoder(probas, proba_sizes, labels.contiguous(), label_sizes)\n",
    "\n",
    "        hypotheses.extend(preds_seq)\n",
    "        references.extend(label_seq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.scorer import Scorer\n",
    "\n",
    "bleu = Scorer.get_moses_multi_bleu(hypotheses, references, lowercase=False)\n",
    "wer, cer = Scorer.get_wer_cer(hypotheses, references)\n",
    "acc = Scorer.get_acc(hypotheses, references)\n",
    "\n",
    "\n",
    "print('Test Summary \\n'\n",
    "            'Bleu: {bleu:.3f}\\n'\n",
    "            'WER:  {wer:.3f}\\n'\n",
    "            'CER:  {cer:.3f}\\n'\n",
    "            'ACC:  {acc:.3f}'.format(bleu=bleu, wer=wer * 100, cer=cer * 100, acc=acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "\n",
    "line_len = sorted([len(l[0]) for l in bad_lines], reverse=True)\n",
    "\n",
    "if max(line_len):\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_xticks(range(1, max(line_len)))\n",
    "\n",
    "    ax = plt.hist(line_len, bins=range(1, max(line_len)))\n",
    "\n",
    "    plt.xlabel('Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(\"Bad Line Length Distribution\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
